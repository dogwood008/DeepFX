{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Python] Keras-RLで簡単に強化学習(DQN)を試す](http://qiita.com/inoory/items/e63ade6f21766c7c2393)を参考に、エージェントを作成する。FXの自動取引を行い、利益を出すのが目標。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import enum\n",
    "from logging import getLogger, StreamHandler, DEBUG, INFO\n",
    "import time\n",
    "import keras\n",
    "import os\n",
    "import warnings\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     12
    ]
   },
   "outputs": [],
   "source": [
    "logger = getLogger(__name__)\n",
    "handler = StreamHandler()\n",
    "handler.setLevel(INFO)\n",
    "logger.setLevel(INFO)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "class DebugTools:\n",
    "    def now():\n",
    "        return dt.datetime.now() + dt.timedelta(hours=9)\n",
    "    def now_str():    \n",
    "        return DebugTools.now().strftime('%y/%m/%d %H:%M:%S')\n",
    "\n",
    "class Action(enum.Enum):\n",
    "    SELL = -1; STAY = 0; BUY = +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HistData:\n",
    "    def __init__(self, date_range=None):\n",
    "        self.csv_path = 'historical_data/USDJPY.hst_.csv'\n",
    "        self.csv_data = pd.read_csv(self.csv_path, index_col=0, parse_dates=True, header=0)\n",
    "        self.date_range = date_range\n",
    "        \n",
    "    def set_date_range(self, date_range):\n",
    "        self.date_range = date_range\n",
    "\n",
    "    def data(self):\n",
    "        if self.date_range is None:\n",
    "            return self.csv_data\n",
    "        else:\n",
    "            return self.csv_data[self.date_range]\n",
    "\n",
    "    def max_value(self):\n",
    "        return self.data()[['High']].max()['High']\n",
    "\n",
    "    def min_value(self):\n",
    "        return self.data()[['Low']].min()['Low']\n",
    "\n",
    "    def dates(self):\n",
    "        return self.data().index.values\n",
    "\n",
    "    ''' 引数の日時がデータフレームに含まれるか '''\n",
    "    def has_datetime(self, datetime64_value):\n",
    "        try:\n",
    "            h.data().loc[datetime64_value]\n",
    "            return True\n",
    "        except KeyError:\n",
    "            return False\n",
    "\n",
    "    def _get_nearist_index(self, before_or_after, datetime):\n",
    "        if before_or_after == 'before':\n",
    "            offset = -1\n",
    "        else:\n",
    "            offset = 0\n",
    "        index = max(h.data().index.searchsorted(datetime) + offset, 0)\n",
    "        return self.data().ix[self.data().index[index]]\n",
    "\n",
    "    ''' 引数の日時を含まない直前に存在する値を取得する '''        \n",
    "    def get_last_exist_datetime(self, datetime64_value):\n",
    "        return self._get_nearist_index('before', datetime64_value)\n",
    "        \n",
    "    ''' 引数の日時を含む直後に存在する値を取得する '''\n",
    "    def get_next_exist_datetime(self, datetime64_value):\n",
    "        return self._get_nearist_index('after', datetime64_value)\n",
    "    \n",
    "    ''' fromとtoの日時の差が閾値内にあるか否か '''\n",
    "    def is_datetime_diff_in_threshould(self, from_datetime, to_datetime, threshold_timedelta):\n",
    "        last_datetime = h.get_last_exist_datetime(from_datetime)\n",
    "        next_exist_datetime = h.get_next_exist_datetime(to_datetime)\n",
    "        delta = next_exist_datetime.name - last_datetime.name\n",
    "        return delta <= threshold_timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' ポジション '''\n",
    "class Position:\n",
    "    def __init__(self, buy_or_sell, price, amount):\n",
    "        self.price = price\n",
    "        self.amount = amount\n",
    "        self.buy_or_sell = buy_or_sell\n",
    "    \n",
    "    ''' 総利益を計算する '''\n",
    "    def calc_profit_by(self, now_price):\n",
    "        return self._calc_unit_profit_by(now_price) * self.amount\n",
    "\n",
    "    ''' 単位あたりの利益を計算する '''\n",
    "    def _calc_unit_profit_by(self, now_price):\n",
    "        if self.buy_or_sell == 'buy' or self.buy_or_sell == Action.BUY.value:\n",
    "            return now_price - self.price\n",
    "        else:\n",
    "            return self.price - now_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FXTrade(gym.core.Env):\n",
    "    AMOUNT_UNIT = 50000\n",
    "    THRESHOULD_TIME_DELTA = dt.timedelta(days=1)\n",
    "    \n",
    "    def __init__(self, initial_cash, spread, hist_data, seed_value=100000, logger=None):\n",
    "        self.hist_data = hist_data\n",
    "        self.initial_cash = initial_cash\n",
    "        self.cash = initial_cash\n",
    "        self.spread = spread\n",
    "        self._positions = []\n",
    "        self._max_date = self._datetime2float(hist_data.dates().max())\n",
    "        self._min_date = self._datetime2float(hist_data.dates().min())\n",
    "        self._seed = seed_value\n",
    "        self._logger = logger\n",
    "        np.random.seed(seed_value)\n",
    "\n",
    "        high = np.array([self._max_date, hist_data.max_value()])\n",
    "        low = np.array([self._min_date, hist_data.min_value()])\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        self.observation_space = gym.spaces.Box(low = low, high = high) # DateFrame, Close prise\n",
    "        \n",
    "    def get_now_datetime_as(self, datetime_or_float):\n",
    "        if datetime_or_float == 'float':\n",
    "            return self._now_datetime\n",
    "        else:\n",
    "            dt = self._float2datetime(self._now_datetime)\n",
    "            return dt\n",
    "    \n",
    "    def _set_now_datetime(self, value):\n",
    "        if isinstance(value, float):\n",
    "            assert self._min_date <= value, value\n",
    "            assert value <= self._max_date, value\n",
    "            self._now_datetime = value\n",
    "            return value\n",
    "        else:\n",
    "            assert self._min_date <= self._datetime2float(value), '%f <= %f, %s' % (self._min_date, self._datetime2float(value), value)\n",
    "            assert self._datetime2float(value) <= self._max_date, '%f <= %f, %s' % (self._datetime2float(value), self._max_date, value)\n",
    "            float_val = self._datetime2float(value)\n",
    "            self._now_datetime = float_val\n",
    "            return float_val\n",
    "            \n",
    "    def setseed(self, seed_value):\n",
    "        self._seed = seed_value\n",
    "        print('Set seed value: %d' % self._seed)\n",
    "        return seed_value\n",
    "        \n",
    "    def _seed(self):\n",
    "        return self._seed\n",
    "    \n",
    "    def _datetime2float(self, datetime64_value):\n",
    "        try:\n",
    "            float_val = float(str(datetime64_value.astype('uint64'))[:10])\n",
    "            return float_val\n",
    "        except:\n",
    "            logger.error('_datetime2float except')\n",
    "            import pdb; pdb.set_trace()\n",
    "    \n",
    "    def _float2datetime(self, float_timestamp):\n",
    "        try:\n",
    "            datetime_val = np.datetime64(dt.datetime.utcfromtimestamp(float_timestamp))\n",
    "            return datetime_val\n",
    "        except:\n",
    "            logger.error('_float2datetime except')\n",
    "            import pdb; pdb.set_trace()\n",
    "    \n",
    "    ''' 総含み益を計算する '''\n",
    "    def _calc_total_unrealized_gain_by(self, now_buy_price, now_sell_price):\n",
    "        positions_buy_or_sell = None\n",
    "        if self._positions:\n",
    "            self._logger.debug('現在の総含み益を再計算')\n",
    "            positions_buy_or_sell = self._positions[0].buy_or_sell\n",
    "            self._logger.debug('buy_or_sell: %d' % positions_buy_or_sell)\n",
    "        else:\n",
    "            positions_buy_or_sell = Action.BUY.value\n",
    "        self._logger.debug('positions_buy_or_sell: %d', positions_buy_or_sell)\n",
    "        now_price_for_positions = self._get_price_of(positions_buy_or_sell, now_buy_price, now_sell_price)\n",
    "        \n",
    "        if not self._positions: # positions is empty\n",
    "            return 0\n",
    "        total_profit = 0\n",
    "        for position in self._positions:\n",
    "            total_profit += position.calc_profit_by(now_price_for_positions)\n",
    "        return total_profit\n",
    "    \n",
    "    ''' 全ポジションを決済する '''\n",
    "    def _close_all_positions_by(self, now_price):\n",
    "        total_profit = 0\n",
    "        buy_or_sell = self._positions[0].buy_or_sell\n",
    "        \n",
    "        for position in self._positions:\n",
    "            total_profit += position.calc_profit_by(now_price)\n",
    "        self._positions = []\n",
    "        self.cash += total_profit\n",
    "        return total_profit\n",
    "        \n",
    "    ''' 注文を出す '''\n",
    "    def _order(self, buy_or_sell, now_price, amount):\n",
    "        position = Position(buy_or_sell=buy_or_sell, price=now_price, amount=amount)\n",
    "        self._positions.append(position)\n",
    "        return position\n",
    "    \n",
    "    ''' 参照すべき価格を返す。取引しようとしているのが売りか買いかで判断する。 '''\n",
    "    def _get_price_of(self, buy_or_sell, now_buy_price, now_sell_price):\n",
    "        if buy_or_sell == Action.BUY.value or buy_or_sell == Action.STAY.value:\n",
    "            return now_buy_price\n",
    "        elif buy_or_sell == Action.SELL.value:\n",
    "            return now_sell_price\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    ''' 今注目している日時を1分進める '''\n",
    "    def _increment_datetime(self):\n",
    "        self._logger.debug('今注目している日時を更新 (=インデックスのインクリメント)')\n",
    "        before_datetime = self.hist_data.data().iloc[[self._now_index], :].index[0]\n",
    "        self._logger.debug('  before: %06d [%s]' % (self._now_index, before_datetime))\n",
    "        self._now_index += 1\n",
    "        after_datetime = self.hist_data.data().iloc[[self._now_index], :].index[0]\n",
    "        self._logger.debug('   after: %06d [%s]' % (self._now_index, after_datetime))\n",
    "        \n",
    "    ''' For Debug: 毎日00:00に買値を表示する。学習の進捗を確認するため。 '''\n",
    "    def _print_if_a_day_begins(self, now_datetime, now_buy_price):\n",
    "        if now_datetime.hour == 0 and now_datetime.minute == 0:\n",
    "            logger.info('%s %f' % (now_datetime, now_buy_price))\n",
    "    \n",
    "    ''' ポジションの手仕舞い、または追加オーダーをする '''\n",
    "    def _close_or_more_order(self, buy_or_sell_or_stay, now_price):\n",
    "        if not self._positions: # position is empty\n",
    "            if buy_or_sell_or_stay != Action.STAY.value:\n",
    "                self._order(buy_or_sell_or_stay, self.AMOUNT_UNIT, now_price)\n",
    "        else: # I have positions\n",
    "            # 売り: -1 / 買い: +1のため、(-1)の乗算で逆のアクションになる\n",
    "            reverse_action = buy_or_sell_or_stay * (-1)\n",
    "            if self._positions[0].buy_or_sell == reverse_action:\n",
    "                # ポジションと逆のアクションを指定されれば、手仕舞い\n",
    "                self._close_all_positions_by(now_price)\n",
    "            else:\n",
    "                # 追加オーダー\n",
    "                self._order(buy_or_sell_or_stay, self.AMOUNT_UNIT, now_price)\n",
    "        \n",
    "    ''' 各stepごとに呼ばれる\n",
    "        actionを受け取り、次のstateとreward、episodeが終了したかどうかを返すように実装 '''\n",
    "    def _step(self, action):\n",
    "        self._logger.debug('_step %06d STARTED' % self._now_index)\n",
    "        \n",
    "        # actionを受け取り、次のstateを決定\n",
    "        buy_or_sell_or_stay = action - 1\n",
    "        assert buy_or_sell_or_stay == -1 or \\\n",
    "            buy_or_sell_or_stay == 0 or \\\n",
    "            buy_or_sell_or_stay == 1, 'buy_or_sell_or_stay: %d' % buy_or_sell_or_stay\n",
    "        \n",
    "        # 今注目している日時を更新\n",
    "        self._increment_datetime()\n",
    "        \n",
    "        # その時点における値群\n",
    "        now_buy_price = self.hist_data.data().ix[[self._now_index], ['Close']].Close.iloc[0]\n",
    "        now_sell_price = now_buy_price - self.spread\n",
    "        \n",
    "        # For Debug: 毎日00:00に買値を表示する。学習の進捗を確認するため。\n",
    "        now_datetime = self.hist_data.data().iloc[[self._now_index], :].index[0]\n",
    "        self._print_if_a_day_begins(now_datetime, now_buy_price)\n",
    "        \n",
    "        # actionによって、使用する価格を変える（売価/買価）\n",
    "        now_price = self._get_price_of(buy_or_sell_or_stay,\n",
    "                                       now_buy_price = now_buy_price,\n",
    "                                       now_sell_price = now_sell_price)\n",
    "        \n",
    "        # ポジションの手仕舞い、または追加オーダーをする\n",
    "        self._close_or_more_order(buy_or_sell_or_stay, now_price)\n",
    "        \n",
    "        # 現在の総含み益の合計値を再計算\n",
    "        total_unrealized_gain = self._calc_total_unrealized_gain_by(\n",
    "            now_buy_price, now_sell_price)\n",
    "\n",
    "        # 日付が学習データの最後と一致すれば終了\n",
    "        done = self._now_index >= len(self.hist_data.data()) - 1\n",
    "        if done:\n",
    "            print('now_datetime: %s' % now_datetime)\n",
    "            print('len(self.hist_data.data()) - 1: %d' % (len(self.hist_data.data()) - 1))\n",
    "\n",
    "        # 報酬は現金と総含み益\n",
    "        reward = total_unrealized_gain + self.cash\n",
    "        \n",
    "        # 次のstate、reward、終了したかどうか、追加情報の順に返す\n",
    "        # 追加情報は特にないので空dict\n",
    "        self._logger.debug('_step ENDED')\n",
    "        return np.array([self._now_index, now_buy_price]), reward, done, {}\n",
    "        \n",
    "    ''' 各episodeの開始時に呼ばれ、初期stateを返すように実装 '''\n",
    "    def _reset(self):\n",
    "        print('_reset START')\n",
    "        print('self._seed: %i' % self._seed)\n",
    "        initial_index = 0\n",
    "        \n",
    "        print('Start datetime: %s' % self.hist_data.dates()[initial_index])\n",
    "        now_buy_price = self.hist_data.data().ix[[initial_index], ['Close']].Close.iloc[0]\n",
    "        self._now_index = initial_index\n",
    "        self._positions = []\n",
    "        print('_reset END')\n",
    "        next_state = [self._now_index, now_buy_price]\n",
    "        return np.array(next_state)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rl.callbacks\n",
    "class ModelSaver(rl.callbacks.TrainEpisodeLogger):\n",
    "    def __init__(self, filepath, monitor='loss', verbose=1, save_best_only=True, mode='min', save_weights_only=False):\n",
    "        if filepath is None:\n",
    "            raise ValueError('Give value to filepath. (Given: %s)' % filepath)\n",
    "        self.best_monitor_value = None\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.verbose = verbose\n",
    "        self.save_best_only = save_best_only\n",
    "        self.mode = mode\n",
    "        self.save_weights_only = save_weights_only\n",
    "        if mode not in ('min', 'max'):\n",
    "            raise ValueError(\"Give 'min' or 'max' to mode. (Given: %s)\" % mode)\n",
    "        self.mode = mode\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "    def on_episode_end(self, episode, logs):\n",
    "        print('========== Model Saver output ==============')\n",
    "        monitor_value = self._formatted_metrics(episode)[self.monitor]\n",
    "\n",
    "        #try:\n",
    "        print('%s value: %e' % (self.monitor, monitor_value))\n",
    "        values = {'episode': episode, self.monitor: monitor_value}\n",
    "        if not self.save_best_only:\n",
    "            values['previous_monitor'] = monitor_value\n",
    "            self._save_model(values)            \n",
    "        elif self.best_monitor_value is None or self._is_this_episode_improved(monitor_value):\n",
    "            previous_value = self.best_monitor_value\n",
    "            self.best_monitor_value = monitor_value\n",
    "            values['previous_monitor'] = previous_value\n",
    "            self._save_model(values)\n",
    "            print('%s %s value: %e' % (self.mode, self.monitor, self.best_monitor_value))\n",
    "        #except:\n",
    "        #    print('Not a float value given.')\n",
    "        print('========== /Model Saver output =============')\n",
    "        super().on_episode_end(episode, logs)\n",
    "\n",
    "    def _is_this_episode_improved(self, monitor_value):\n",
    "        if self.mode == 'min':\n",
    "            return monitor_value < self.best_monitor_value\n",
    "        else:\n",
    "            return monitor_value > self.best_monitor_value\n",
    "        \n",
    "    def _save_model(self, kwargs):\n",
    "        previous_monitor = kwargs['previous_monitor']\n",
    "        filepath = self.filepath.format_map(kwargs)\n",
    "        if self.verbose > 0:\n",
    "            print(\"Step %05d: model improved\\n  from %e\\n    to %e,\"\n",
    "                  ' saving model to %s'\n",
    "                  % (self.step, previous_monitor or 0.0,\n",
    "                     self.best_monitor_value or 0.0, filepath))\n",
    "        if self.save_weights_only:\n",
    "            self.model.save_weights(filepath + '.hdf5', overwrite=True)\n",
    "            print('Save weights to %s has done.' % filepath)\n",
    "        else:\n",
    "            self.model.model.save(filepath + '.h5', overwrite=True)\n",
    "            print('Save model to %s has done.' % filepath)\n",
    "\n",
    "    def _formatted_metrics(self, episode):\n",
    "        # Format all metrics.\n",
    "        metrics = np.array(self.metrics[episode])\n",
    "        metrics_variables = []\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('error')\n",
    "            for idx, name in enumerate(self.metrics_names):\n",
    "                try:\n",
    "                    value = np.nanmean(metrics[:, idx])\n",
    "                except Warning:\n",
    "                    if name == 'loss':\n",
    "                        value = float('inf')\n",
    "                    else:\n",
    "                        value = '--'\n",
    "                metrics_variables += [name, value]\n",
    "        return dict(itertools.zip_longest(*[iter(metrics_variables)] * 2, fillvalue=\"\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "''' 元のTensorBoardだと、value.item()で死ぬのでvalueに変更。変更点はここだけ。 '''\n",
    "class MyTensorBoard(keras.callbacks.TensorBoard):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.validation_data and self.histogram_freq:\n",
    "            if epoch % self.histogram_freq == 0:\n",
    "                # TODO: implement batched calls to sess.run\n",
    "                # (current call will likely go OOM on GPU)\n",
    "                if self.model.uses_learning_phase:\n",
    "                    cut_v_data = len(self.model.inputs)\n",
    "                    val_data = self.validation_data[:cut_v_data] + [0]\n",
    "                    tensors = self.model.inputs + [K.learning_phase()]\n",
    "                else:\n",
    "                    val_data = self.validation_data\n",
    "                    tensors = self.model.inputs\n",
    "                feed_dict = dict(zip(tensors, val_data))\n",
    "                result = self.sess.run([self.merged], feed_dict=feed_dict)\n",
    "                summary_str = result[0]\n",
    "                self.writer.add_summary(summary_str, epoch)\n",
    "\n",
    "        if self.embeddings_freq and self.embeddings_logs:\n",
    "            if epoch % self.embeddings_freq == 0:\n",
    "                for log in self.embeddings_logs:\n",
    "                    self.saver.save(self.sess, log, epoch)\n",
    "\n",
    "        for name, value in logs.items():\n",
    "            if name in ['batch', 'size']:\n",
    "                continue\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value # Modified from: value.item()\n",
    "            summary_value.tag = name\n",
    "            self.writer.add_summary(summary, epoch)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     8
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "class DeepFX:\n",
    "    def __init__(self, env, mode, nb_steps=None,\n",
    "              log_directory='./logs', model_directory='./models',\n",
    "              model_filename='Keras-RL_DQN_FX_model_meanq{mean_q:e}_episode{episode:05d}',\n",
    "              prepared_model_filename=None,\n",
    "              weights_filename='Keras-RL_DQN_FX_weights.h5',):\n",
    "\n",
    "        self._log_directory = log_directory\n",
    "        self._model_directory = model_directory\n",
    "        self._model_filename = model_filename\n",
    "        self._prepared_model_filename = prepared_model_filename\n",
    "        self._weights_filename = weights_filename\n",
    "        self._load_model_path = self._relative_path(model_directory, prepared_model_filename) \n",
    "        self._save_model_path = self._relative_path(model_directory, model_filename)\n",
    "        self._env = env\n",
    "\n",
    "    def setup(self):\n",
    "        self._agent, self._model, self._memory, self._policy = self._initialize_agent()\n",
    "        self._agent.compile('adam')\n",
    "        print(self._model.summary())\n",
    "\n",
    "    def train(self, is_for_time_measurement=False, wipe_instance_variables_after=True):\n",
    "        self.setup()\n",
    "        self._callbacks = self._get_callbacks()\n",
    "        self._fit(self._agent, is_for_time_measurement, self._env, self._callbacks)\n",
    "        if wipe_instance_variables_after:\n",
    "            self._wipe_instance_variables()\n",
    "\n",
    "    def test(self, episodes, callbacks=[], wipe_instance_variables_after=True):\n",
    "        self.setup()\n",
    "        self._agent.test(self._env, nb_episodes=episodes, visualize=False, callbacks=callbacks)\n",
    "\n",
    "        %matplotlib inline\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        for obs in callbacks[0].rewards.values():\n",
    "            plt.plot([o for o in obs])\n",
    "        plt.xlabel(\"step\")\n",
    "        plt.ylabel(\"reward\")\n",
    "        if wipe_instance_variables_after:\n",
    "            self._wipe_instance_variables()\n",
    "        \n",
    "    def _wipe_instance_variables(self):\n",
    "         self._callbacks, self._agent, self._model, \\\n",
    "                self._memory, self._policy, self.env = [None] * 6\n",
    "        \n",
    "    def _relative_path(self, directory, filename):\n",
    "        if directory is None or filename is None:\n",
    "            return None\n",
    "        return os.path.join(directory, filename)\n",
    "\n",
    "    def _get_model(self, load_model_path, observation_space_shape, nb_actions):\n",
    "        if load_model_path is None:\n",
    "            # DQNのネットワーク定義\n",
    "            model = Sequential()\n",
    "            model.add(Flatten(input_shape=(1,) + observation_space_shape))\n",
    "        #    model.add(Dense(4))\n",
    "        #    model.add(Activation('relu'))\n",
    "        #    model.add(Dense(4))\n",
    "        #    model.add(Activation('relu'))\n",
    "            model.add(Dense(nb_actions))\n",
    "            model.add(Activation('relu'))\n",
    "        else:\n",
    "            model = keras.models.load_model(load_model_path)\n",
    "        return model\n",
    "\n",
    "    def _initialize_agent(self):\n",
    "        nb_actions = self._env.action_space.n\n",
    "        observation_space_shape = self._env.observation_space.shape\n",
    "        model = self._get_model(self._load_model_path, observation_space_shape, nb_actions)\n",
    "        \n",
    "        # experience replay用のmemory\n",
    "        memory = SequentialMemory(limit=500000, window_length=1)\n",
    "        # 行動方策はオーソドックスなepsilon-greedy。ほかに、各行動のQ値によって確率を決定するBoltzmannQPolicyが利用可能\n",
    "        policy = EpsGreedyQPolicy(eps=0.1) \n",
    "        dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n",
    "                       policy=policy)\n",
    "                       #target_model_update=1e-2, policy=policy)\n",
    "        #dqn.compile(Adam(lr=1e-3))\n",
    "        return (dqn, model, memory, policy)\n",
    "        \n",
    "    def _get_callbacks(self):\n",
    "        tensor_board_callback = MyTensorBoard(log_dir=self._log_directory, histogram_freq=1, embeddings_layer_names=True, write_graph=True)\n",
    "        model_saver_callback = ModelSaver(self._save_model_path, monitor='mean_q', mode='max')\n",
    "        callbacks = [tensor_board_callback, model_saver_callback]\n",
    "        return callbacks\n",
    "\n",
    "    def _fit(self, agent, is_for_time_measurement, env, callbacks=[]):\n",
    "        if is_for_time_measurement:\n",
    "            start = time.time()\n",
    "            print(DebugTools.now_str())\n",
    "            #minutes = 2591940/60 # 2591940secs = '2010-09-30 23:59:00' - '2010-09-01 00:00:00'\n",
    "            #minutes = (60 * 24 - 1) * 1# a day * 1\n",
    "            minutes = (60 * 24 - 1) * 10# a day * 10\n",
    "            #minutes = (60 * 24 - 1) * 2 # 2days\n",
    "            #minutes = (60 * 24 - 1) * 10 * 9999999 # 10days * 9999999 Epochs\n",
    "            #minutes = (60 * 24 - 1) * 30 * 9999999# 30days * 9999999 Epochs\n",
    "            history = agent.fit(env, nb_steps=minutes, visualize=False, verbose=2, nb_max_episode_steps=None, \\\n",
    "                             callbacks=callbacks)\n",
    "            elapsed_time = time.time() - start\n",
    "            print((\"elapsed_time:{0}\".format(elapsed_time)) + \"[sec]\")\n",
    "            print(DebugTools.now_str())\n",
    "        else:\n",
    "            history = agent.fit(env, nb_steps=50000, visualize=False, verbose=2, nb_max_episode_steps=None)\n",
    "        #学習の様子を描画したいときは、Envに_render()を実装して、visualize=True にします,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = HistData('2010/9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EpisodeLogger(rl.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.observations = {}\n",
    "        self.rewards = {}\n",
    "        self.actions = {}\n",
    "\n",
    "    def on_episode_begin(self, episode, logs):\n",
    "        self.observations[episode] = []\n",
    "        self.rewards[episode] = []\n",
    "        self.actions[episode] = []\n",
    "\n",
    "    def on_step_end(self, step, logs):\n",
    "        episode = logs['episode']\n",
    "        self.observations[episode].append(logs['observation'])\n",
    "        self.rewards[episode].append(logs['reward'])\n",
    "        self.actions[episode].append(logs['action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FXTrade(1000000, 0.08, h, logger=logger)\n",
    "#env = FXTrade(1000000, 0.08, h, logger=logger)\n",
    "prepared_model_filename = None #'Keras-RL_DQN_FX_model_meanq1.440944e+06_episode00003.h5'\n",
    "dfx = DeepFX(env, 'test', prepared_model_filename=prepared_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_to_train = True\n",
    "if is_to_train:\n",
    "    dfx.train(is_for_time_measurement=True)\n",
    "else:\n",
    "    dfx.test(1, [EpisodeLogger()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h.data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
